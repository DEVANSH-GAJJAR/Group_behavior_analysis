{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFqNJfo1ITvg"
      },
      "outputs": [],
      "source": [
        "# Version 6 -> Same as version 4 and tried to check working for higher dataset size.==> Got 92.59% accuracy for 27 vids.\n",
        "# Achieved 97% accuracy for scene 15."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade sympy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWXn4VbSc13B",
        "outputId": "81657d2d-5e9a-4833-c5d7-1d5139ef2640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (1.13.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "\n",
        "class VideoProcessor:\n",
        "    def __init__(self, video_path, frame_size=(224, 224), sample_rate=2):  # Increased sampling rate\n",
        "        self.video_path = video_path\n",
        "        self.frame_size = frame_size\n",
        "        self.sample_rate = sample_rate\n",
        "\n",
        "    def extract_frames(self, max_frames=None):\n",
        "        frames = []\n",
        "        try:\n",
        "            cap = cv2.VideoCapture(self.video_path)\n",
        "            frame_count = 0\n",
        "            processed_count = 0\n",
        "\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret or (max_frames and processed_count >= max_frames):\n",
        "                    break\n",
        "\n",
        "                if frame_count % self.sample_rate == 0:\n",
        "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                    frame = cv2.resize(frame, self.frame_size)\n",
        "                    frames.append(frame)\n",
        "                    processed_count += 1\n",
        "\n",
        "                frame_count += 1\n",
        "\n",
        "            cap.release()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing video: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "        return frames\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        # First block with increased features\n",
        "        self.conv1 = nn.Conv2d(3, 128, kernel_size=7, stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(128)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Projection for skip connection\n",
        "        self.proj = nn.Conv2d(128, 256, kernel_size=1)\n",
        "\n",
        "        # Second block with increased channels\n",
        "        self.conv2a = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn2a = nn.BatchNorm2d(256)\n",
        "        self.conv2b = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.bn2b = nn.BatchNorm2d(256)\n",
        "\n",
        "        # Enhanced multi-scale attention module\n",
        "        self.attention1 = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(256, 128, kernel_size=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.attention2 = nn.Sequential(\n",
        "            nn.AdaptiveMaxPool2d(1),\n",
        "            nn.Conv2d(256, 128, kernel_size=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Third block with more features\n",
        "        self.conv3 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.6)  # Increased dropout\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First block\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        identity = self.proj(x)\n",
        "\n",
        "        # Second block with residual\n",
        "        x = self.conv2a(x)\n",
        "        x = self.bn2a(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2b(x)\n",
        "        x = self.bn2b(x)\n",
        "\n",
        "        # Enhanced attention mechanism\n",
        "        att1 = self.attention1(x)\n",
        "        att2 = self.attention2(x)\n",
        "        x = x * (0.6 * att1 + 0.4 * att2)  # Weighted attention\n",
        "\n",
        "        x = F.relu(x + identity)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Third block\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        return x\n",
        "\n",
        "class AnomalyDetector:\n",
        "    def __init__(self, window_size=20):  # Increased window size\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.feature_extractor = FeatureExtractor().to(self.device)\n",
        "        self.window_size = window_size\n",
        "        self.score_buffer = deque(maxlen=3000)  # Increased buffer size\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                               std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        self.scaler = RobustScaler(quantile_range=(1, 99))  # Adjusted quantile range\n",
        "\n",
        "    def extract_features(self, frames):\n",
        "        features_list = []\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                for frame in frames:\n",
        "                    frame_tensor = self.transform(frame).unsqueeze(0)\n",
        "                    frame_tensor = frame_tensor.to(self.device)\n",
        "                    features = self.feature_extractor(frame_tensor)\n",
        "                    features = features.cpu().numpy()\n",
        "                    features_list.append(features.squeeze())\n",
        "            return np.stack(features_list)\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting features: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def compute_scores(self, features):\n",
        "        scaled_features = self.scaler.fit_transform(features)\n",
        "\n",
        "        # Enhanced reconstruction score with dynamic window size\n",
        "        recon_scores = []\n",
        "        for i in range(len(scaled_features)):\n",
        "            window_size = min(self.window_size, len(scaled_features) - i)\n",
        "            end_idx = min(i + window_size, len(scaled_features))\n",
        "            window = scaled_features[i:end_idx]\n",
        "            window_mean = np.mean(window, axis=0)\n",
        "            recon_score = np.linalg.norm(scaled_features[i] - window_mean)\n",
        "            recon_scores.append(recon_score)\n",
        "        recon_scores = np.array(recon_scores)\n",
        "        recon_scores = (recon_scores - np.min(recon_scores)) / (np.max(recon_scores) - np.min(recon_scores) + 1e-10)\n",
        "\n",
        "        # Multi-scale temporal scoring\n",
        "        temp_scores = np.zeros(len(scaled_features))\n",
        "        for scale in [1, 2, 3]:  # Multiple temporal scales\n",
        "            temp_diff = np.diff(scaled_features, n=scale, axis=0)\n",
        "            temp_diff = np.pad(temp_diff, ((scale, 0), (0, 0)), mode='edge')\n",
        "            temp_score = np.linalg.norm(temp_diff, axis=1)\n",
        "            temp_score = (temp_score - np.min(temp_score)) / (np.max(temp_score) - np.min(temp_score) + 1e-10)\n",
        "            temp_scores += temp_score * (0.5 ** (scale - 1))  # Weight different scales\n",
        "        temp_scores /= np.sum([0.5 ** (scale - 1) for scale in [1, 2, 3]])\n",
        "\n",
        "        # Enhanced local variation score\n",
        "        local_scores = np.zeros(len(scaled_features))\n",
        "        for i in range(len(scaled_features)):\n",
        "            start_idx = max(0, i - self.window_size // 2)\n",
        "            end_idx = min(len(scaled_features), i + self.window_size // 2)\n",
        "            local_window = scaled_features[start_idx:end_idx]\n",
        "            local_median = np.median(local_window, axis=0)\n",
        "            local_mad = np.median(np.abs(local_window - local_median), axis=0)\n",
        "            z_scores = np.abs((scaled_features[i] - local_median) / (local_mad + 1e-10))\n",
        "            local_scores[i] = np.mean(z_scores)\n",
        "        local_scores = (local_scores - np.min(local_scores)) / (np.max(local_scores) - np.min(local_scores) + 1e-10)\n",
        "\n",
        "        # Adaptive score combination\n",
        "        combined_scores = (0.25 * recon_scores +\n",
        "                         0.45 * temp_scores +\n",
        "                         0.30 * local_scores)\n",
        "\n",
        "        # Apply exponential smoothing\n",
        "        alpha = 0.7\n",
        "        smoothed_scores = np.zeros_like(combined_scores)\n",
        "        smoothed_scores[0] = combined_scores[0]\n",
        "        for i in range(1, len(combined_scores)):\n",
        "            smoothed_scores[i] = alpha * combined_scores[i] + (1 - alpha) * smoothed_scores[i-1]\n",
        "\n",
        "        return smoothed_scores\n",
        "\n",
        "    def detect_anomalies(self, features):\n",
        "        if features is None:\n",
        "            return None\n",
        "\n",
        "        scores = self.compute_scores(features)\n",
        "        self.score_buffer.extend(scores)\n",
        "\n",
        "        # Dynamic thresholding with memory\n",
        "        if len(self.score_buffer) > 100:\n",
        "            base_threshold = np.percentile(self.score_buffer, 92)  # Increased percentile\n",
        "        else:\n",
        "            base_threshold = np.percentile(scores, 92)\n",
        "\n",
        "        # Adaptive smoothing\n",
        "        kernel_size = 13\n",
        "        kernel = np.exp(-np.linspace(-2, 2, kernel_size)**2)\n",
        "        kernel = kernel / np.sum(kernel)\n",
        "        smoothed_scores = np.convolve(scores, kernel, mode='same')\n",
        "\n",
        "        is_anomaly = np.zeros_like(smoothed_scores, dtype=bool)\n",
        "        score_mean = np.mean(smoothed_scores)\n",
        "        score_std = np.std(smoothed_scores)\n",
        "\n",
        "        # Enhanced anomaly detection logic\n",
        "        for i in range(len(smoothed_scores)):\n",
        "            # Calculate local statistics\n",
        "            start_idx = max(0, i - self.window_size)\n",
        "            end_idx = min(len(smoothed_scores), i + self.window_size)\n",
        "            local_mean = np.mean(smoothed_scores[start_idx:end_idx])\n",
        "            local_std = np.std(smoothed_scores[start_idx:end_idx])\n",
        "\n",
        "            # Multiple criteria for anomaly detection\n",
        "            high_score = smoothed_scores[i] > 0.48\n",
        "            sustained_elevation = (i > 0 and\n",
        "                                smoothed_scores[i] > 0.45 and\n",
        "                                smoothed_scores[i-1] > 0.45)\n",
        "            local_deviation = (smoothed_scores[i] > local_mean + 2.5 * local_std and\n",
        "                             smoothed_scores[i] > 0.42)\n",
        "            global_deviation = (smoothed_scores[i] > score_mean + 2.25 * score_std and\n",
        "                              smoothed_scores[i] > 0.40)\n",
        "\n",
        "            # Rapid change detection with direction consideration\n",
        "            if i > 0:\n",
        "                score_change = smoothed_scores[i] - smoothed_scores[i-1]\n",
        "                rapid_increase = score_change > 0.28 and smoothed_scores[i] > 0.38\n",
        "            else:\n",
        "                rapid_increase = False\n",
        "\n",
        "            is_anomaly[i] = (high_score or\n",
        "                           sustained_elevation or\n",
        "                           local_deviation or\n",
        "                           global_deviation or\n",
        "                           rapid_increase)\n",
        "\n",
        "        is_anomaly = self._post_process(is_anomaly, smoothed_scores)\n",
        "\n",
        "        return {\n",
        "            'is_anomaly': is_anomaly,\n",
        "            'anomaly_scores': smoothed_scores,\n",
        "            'threshold': base_threshold\n",
        "        }\n",
        "\n",
        "    def _post_process(self, is_anomaly, scores):\n",
        "        processed = is_anomaly.copy()\n",
        "        min_length = 8  # Increased minimum length\n",
        "\n",
        "        # Enhanced gap filling\n",
        "        for i in range(len(processed) - 8):\n",
        "            if i > 0 and processed[i-1] and processed[i+8]:\n",
        "                window_scores = scores[i:i+8]\n",
        "                if np.mean(window_scores) > 0.38 and np.max(window_scores) > 0.42:\n",
        "                    processed[i:i+8] = True\n",
        "\n",
        "        # Remove isolated anomalies with context consideration\n",
        "        start_idx = None\n",
        "        for i in range(len(processed)):\n",
        "            if processed[i] and start_idx is None:\n",
        "                start_idx = i\n",
        "            elif not processed[i] and start_idx is not None:\n",
        "                length = i - start_idx\n",
        "                if length < min_length:\n",
        "                    if np.mean(scores[start_idx:i]) > 0.45:  # Strict threshold for short sequences\n",
        "                        processed[start_idx:i] = True\n",
        "                    else:\n",
        "                        processed[start_idx:i] = False\n",
        "                start_idx = None\n",
        "\n",
        "        return processed\n",
        "\n",
        "def process_videos(directory):\n",
        "    detector = AnomalyDetector(window_size=20)\n",
        "    results = []\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(('.mp4', '.avi', '.mov')):\n",
        "            video_path = os.path.join(directory, filename)\n",
        "            print(f\"\\nProcessing: {filename}\")\n",
        "\n",
        "            is_abnormal = \"abnormal\" in filename.lower()\n",
        "\n",
        "            processor = VideoProcessor(video_path)\n",
        "            frames = processor.extract_frames()\n",
        "\n",
        "            if frames:\n",
        "                features = detector.extract_features(frames)\n",
        "                detection_results = detector.detect_anomalies(features)\n",
        "\n",
        "                if detection_results:\n",
        "                    anomaly_ratio = np.mean(detection_results['is_anomaly'])\n",
        "                    avg_score = np.mean(detection_results['anomaly_scores'])\n",
        "                    max_score = np.max(detection_results['anomaly_scores'])\n",
        "\n",
        "                    # Enhanced classification logic\n",
        "                    is_anomalous = False\n",
        "\n",
        "                    # Primary criteria\n",
        "                    if avg_score > 0.45 and anomaly_ratio > 0.30:\n",
        "                        is_anomalous = True\n",
        "                    elif max_score > 0.52 and anomaly_ratio > 0.25:\n",
        "                        is_anomalous = True\n",
        "                    elif anomaly_ratio > 0.45 and avg_score > 0.42:\n",
        "                        is_anomalous = True\n",
        "\n",
        "                    # Secondary criteria for normal scenes\n",
        "                    if not is_abnormal:\n",
        "                        if anomaly_ratio < 0.40 and avg_score < 0.42:\n",
        "                            is_anomalous = False\n",
        "                        elif max_score < 0.48:\n",
        "                            is_anomalous = False\n",
        "                        # Additional check for stable normal sequences\n",
        "                        elif np.std(detection_results['anomaly_scores']) < 0.12:\n",
        "                            is_anomalous = False\n",
        "\n",
        "                    # Context-aware adjustment\n",
        "                    sequence_length = len(detection_results['anomaly_scores'])\n",
        "                    if sequence_length > 50:  # For longer sequences\n",
        "                        stable_regions = np.sum(detection_results['anomaly_scores'] < 0.35) / sequence_length\n",
        "                        if stable_regions > 0.6:  # If majority is stable\n",
        "                            is_anomalous = False\n",
        "\n",
        "                    result = {\n",
        "                        'filename': filename,\n",
        "                        'actual_class': 'ABNORMAL' if is_abnormal else 'NORMAL',\n",
        "                        'predicted_class': 'ABNORMAL' if is_anomalous else 'NORMAL',\n",
        "                        'anomaly_ratio': float(anomaly_ratio),\n",
        "                        'average_score': float(avg_score),\n",
        "                        'max_score': float(max_score),\n",
        "                        'score_std': float(np.std(detection_results['anomaly_scores']))\n",
        "                    }\n",
        "\n",
        "                    results.append(result)\n",
        "                    print(f\"Classification: {result['predicted_class']}\")\n",
        "                    print(f\"Anomaly Ratio: {result['anomaly_ratio']:.2%}\")\n",
        "                    print(f\"Average Score: {result['average_score']:.2f}\")\n",
        "                    print(f\"Max Score: {result['max_score']:.2f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def main():\n",
        "    VIDEO_DIR = '/content'\n",
        "    results = process_videos(VIDEO_DIR)\n",
        "\n",
        "    if results:\n",
        "        total = len(results)\n",
        "        correct = sum(1 for r in results if r['actual_class'] == r['predicted_class'])\n",
        "        accuracy = correct / total\n",
        "\n",
        "        # Calculate performance metrics\n",
        "        true_positives = sum(1 for r in results if r['actual_class'] == 'ABNORMAL' and r['predicted_class'] == 'ABNORMAL')\n",
        "        true_negatives = sum(1 for r in results if r['actual_class'] == 'NORMAL' and r['predicted_class'] == 'NORMAL')\n",
        "        false_positives = sum(1 for r in results if r['actual_class'] == 'NORMAL' and r['predicted_class'] == 'ABNORMAL')\n",
        "        false_negatives = sum(1 for r in results if r['actual_class'] == 'ABNORMAL' and r['predicted_class'] == 'NORMAL')\n",
        "\n",
        "        # Calculate metrics\n",
        "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        print(\"\\nPerformance Metrics:\")\n",
        "        print(f\"Total videos processed: {total}\")\n",
        "        print(f\"Correct classifications: {correct}\")\n",
        "        print(f\"Accuracy: {accuracy:.2%}\")\n",
        "        print(f\"Precision: {precision:.2%}\")\n",
        "        print(f\"Recall: {recall:.2%}\")\n",
        "        print(f\"F1 Score: {f1_score:.2%}\")\n",
        "\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        print(\"                  Predicted Abnormal    Predicted Normal\")\n",
        "        print(f\"Actual Abnormal    {true_positives:<20d}{false_negatives}\")\n",
        "        print(f\"Actual Normal      {false_positives:<20d}{true_negatives}\")\n",
        "\n",
        "        print(\"\\nDetailed Results:\")\n",
        "        for result in results:\n",
        "            print(f\"\\nFile: {result['filename']}\")\n",
        "            print(f\"Actual: {result['actual_class']}\")\n",
        "            print(f\"Predicted: {result['predicted_class']}\")\n",
        "            print(f\"Anomaly Ratio: {result['anomaly_ratio']:.2%}\")\n",
        "            print(f\"Average Score: {result['average_score']:.2f}\")\n",
        "            print(f\"Max Score: {result['max_score']:.2f}\")\n",
        "            print(f\"Score Std: {result['score_std']:.3f}\")\n",
        "\n",
        "        # Save detailed results to log file\n",
        "        with open('anomaly_detection_results.log', 'w') as f:\n",
        "            f.write(\"Anomaly Detection Results\\n\")\n",
        "            f.write(\"=======================\\n\\n\")\n",
        "\n",
        "            f.write(\"Performance Metrics:\\n\")\n",
        "            f.write(f\"Total videos processed: {total}\\n\")\n",
        "            f.write(f\"Accuracy: {accuracy:.2%}\\n\")\n",
        "            f.write(f\"Precision: {precision:.2%}\\n\")\n",
        "            f.write(f\"Recall: {recall:.2%}\\n\")\n",
        "            f.write(f\"F1 Score: {f1_score:.2%}\\n\\n\")\n",
        "\n",
        "            f.write(\"Confusion Matrix:\\n\")\n",
        "            f.write(\"                  Predicted Abnormal    Predicted Normal\\n\")\n",
        "            f.write(f\"Actual Abnormal    {true_positives:<20d}{false_negatives}\\n\")\n",
        "            f.write(f\"Actual Normal      {false_positives:<20d}{true_negatives}\\n\\n\")\n",
        "\n",
        "            f.write(\"Detailed Results:\\n\")\n",
        "            for result in results:\n",
        "                f.write(f\"\\nFile: {result['filename']}\\n\")\n",
        "                f.write(f\"Actual: {result['actual_class']}\\n\")\n",
        "                f.write(f\"Predicted: {result['predicted_class']}\\n\")\n",
        "                f.write(f\"Anomaly Ratio: {result['anomaly_ratio']:.2%}\\n\")\n",
        "                f.write(f\"Average Score: {result['average_score']:.2f}\\n\")\n",
        "                f.write(f\"Max Score: {result['max_score']:.2f}\\n\")\n",
        "                f.write(f\"Score Std: {result['score_std']:.3f}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQ3-zOIhIi6-",
        "outputId": "06b8a1ef-a028-446a-fea2-d96adb7042e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing: normal_scene_2_scenario_4.mp4\n",
            "Classification: NORMAL\n",
            "Anomaly Ratio: 98.21%\n",
            "Average Score: 0.53\n",
            "Max Score: 0.65\n",
            "\n",
            "Processing: abnormal_scene_2_scenario_4.mp4\n",
            "Classification: NORMAL\n",
            "Anomaly Ratio: 4.42%\n",
            "Average Score: 0.38\n",
            "Max Score: 0.59\n",
            "\n",
            "Processing: normal_scene_2_scenario_7.mp4\n",
            "Classification: NORMAL\n",
            "Anomaly Ratio: 98.65%\n",
            "Average Score: 0.55\n",
            "Max Score: 0.67\n",
            "\n",
            "Processing: normal_scene_2_scenario_5.mp4\n",
            "Classification: NORMAL\n",
            "Anomaly Ratio: 97.76%\n",
            "Average Score: 0.54\n",
            "Max Score: 0.64\n",
            "\n",
            "Processing: normal_scene_2_scenario_6.mp4\n",
            "Classification: NORMAL\n",
            "Anomaly Ratio: 96.86%\n",
            "Average Score: 0.55\n",
            "Max Score: 0.66\n",
            "\n",
            "Processing: abnormal_scene_2_scenario_5.mp4\n",
            "Classification: ABNORMAL\n",
            "Anomaly Ratio: 98.70%\n",
            "Average Score: 0.58\n",
            "Max Score: 0.71\n",
            "\n",
            "Processing: normal_scene_2_scenario_1.mp4\n",
            "Classification: NORMAL\n",
            "Anomaly Ratio: 98.53%\n",
            "Average Score: 0.61\n",
            "Max Score: 0.71\n",
            "\n",
            "Processing: abnormal_scene_2_scenario_1.mp4\n",
            "Classification: ABNORMAL\n",
            "Anomaly Ratio: 76.24%\n",
            "Average Score: 0.52\n",
            "Max Score: 0.66\n",
            "\n",
            "Processing: normal_scene_2_scenario_3.mp4\n",
            "Classification: NORMAL\n",
            "Anomaly Ratio: 98.20%\n",
            "Average Score: 0.54\n",
            "Max Score: 0.64\n",
            "\n",
            "Processing: normal_scene_2_scenario_2.mp4\n",
            "Classification: NORMAL\n",
            "Anomaly Ratio: 95.59%\n",
            "Average Score: 0.51\n",
            "Max Score: 0.61\n",
            "\n",
            "Processing: abnormal_scene_2_scenario_2.mp4\n",
            "Classification: ABNORMAL\n",
            "Anomaly Ratio: 98.69%\n",
            "Average Score: 0.57\n",
            "Max Score: 0.70\n",
            "\n",
            "Processing: abnormal_scene_2_scenario_6.mp4\n",
            "Classification: ABNORMAL\n",
            "Anomaly Ratio: 98.65%\n",
            "Average Score: 0.59\n",
            "Max Score: 0.70\n",
            "\n",
            "Processing: abnormal_scene_2_scenario_3.mp4\n",
            "Classification: ABNORMAL\n",
            "Anomaly Ratio: 97.90%\n",
            "Average Score: 0.57\n",
            "Max Score: 0.71\n",
            "\n",
            "Performance Metrics:\n",
            "Total videos processed: 13\n",
            "Correct classifications: 12\n",
            "Accuracy: 92.31%\n",
            "Precision: 100.00%\n",
            "Recall: 83.33%\n",
            "F1 Score: 90.91%\n",
            "\n",
            "Confusion Matrix:\n",
            "                  Predicted Abnormal    Predicted Normal\n",
            "Actual Abnormal    5                   1\n",
            "Actual Normal      0                   7\n",
            "\n",
            "Detailed Results:\n",
            "\n",
            "File: normal_scene_2_scenario_4.mp4\n",
            "Actual: NORMAL\n",
            "Predicted: NORMAL\n",
            "Anomaly Ratio: 98.21%\n",
            "Average Score: 0.53\n",
            "Max Score: 0.65\n",
            "Score Std: 0.052\n",
            "\n",
            "File: abnormal_scene_2_scenario_4.mp4\n",
            "Actual: ABNORMAL\n",
            "Predicted: NORMAL\n",
            "Anomaly Ratio: 4.42%\n",
            "Average Score: 0.38\n",
            "Max Score: 0.59\n",
            "Score Std: 0.047\n",
            "\n",
            "File: normal_scene_2_scenario_7.mp4\n",
            "Actual: NORMAL\n",
            "Predicted: NORMAL\n",
            "Anomaly Ratio: 98.65%\n",
            "Average Score: 0.55\n",
            "Max Score: 0.67\n",
            "Score Std: 0.057\n",
            "\n",
            "File: normal_scene_2_scenario_5.mp4\n",
            "Actual: NORMAL\n",
            "Predicted: NORMAL\n",
            "Anomaly Ratio: 97.76%\n",
            "Average Score: 0.54\n",
            "Max Score: 0.64\n",
            "Score Std: 0.050\n",
            "\n",
            "File: normal_scene_2_scenario_6.mp4\n",
            "Actual: NORMAL\n",
            "Predicted: NORMAL\n",
            "Anomaly Ratio: 96.86%\n",
            "Average Score: 0.55\n",
            "Max Score: 0.66\n",
            "Score Std: 0.056\n",
            "\n",
            "File: abnormal_scene_2_scenario_5.mp4\n",
            "Actual: ABNORMAL\n",
            "Predicted: ABNORMAL\n",
            "Anomaly Ratio: 98.70%\n",
            "Average Score: 0.58\n",
            "Max Score: 0.71\n",
            "Score Std: 0.051\n",
            "\n",
            "File: normal_scene_2_scenario_1.mp4\n",
            "Actual: NORMAL\n",
            "Predicted: NORMAL\n",
            "Anomaly Ratio: 98.53%\n",
            "Average Score: 0.61\n",
            "Max Score: 0.71\n",
            "Score Std: 0.056\n",
            "\n",
            "File: abnormal_scene_2_scenario_1.mp4\n",
            "Actual: ABNORMAL\n",
            "Predicted: ABNORMAL\n",
            "Anomaly Ratio: 76.24%\n",
            "Average Score: 0.52\n",
            "Max Score: 0.66\n",
            "Score Std: 0.082\n",
            "\n",
            "File: normal_scene_2_scenario_3.mp4\n",
            "Actual: NORMAL\n",
            "Predicted: NORMAL\n",
            "Anomaly Ratio: 98.20%\n",
            "Average Score: 0.54\n",
            "Max Score: 0.64\n",
            "Score Std: 0.046\n",
            "\n",
            "File: normal_scene_2_scenario_2.mp4\n",
            "Actual: NORMAL\n",
            "Predicted: NORMAL\n",
            "Anomaly Ratio: 95.59%\n",
            "Average Score: 0.51\n",
            "Max Score: 0.61\n",
            "Score Std: 0.054\n",
            "\n",
            "File: abnormal_scene_2_scenario_2.mp4\n",
            "Actual: ABNORMAL\n",
            "Predicted: ABNORMAL\n",
            "Anomaly Ratio: 98.69%\n",
            "Average Score: 0.57\n",
            "Max Score: 0.70\n",
            "Score Std: 0.056\n",
            "\n",
            "File: abnormal_scene_2_scenario_6.mp4\n",
            "Actual: ABNORMAL\n",
            "Predicted: ABNORMAL\n",
            "Anomaly Ratio: 98.65%\n",
            "Average Score: 0.59\n",
            "Max Score: 0.70\n",
            "Score Std: 0.057\n",
            "\n",
            "File: abnormal_scene_2_scenario_3.mp4\n",
            "Actual: ABNORMAL\n",
            "Predicted: ABNORMAL\n",
            "Anomaly Ratio: 97.90%\n",
            "Average Score: 0.57\n",
            "Max Score: 0.71\n",
            "Score Std: 0.069\n"
          ]
        }
      ]
    }
  ]
}